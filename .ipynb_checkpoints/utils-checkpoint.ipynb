{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d498eea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES NEEDED\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import re\n",
    "\n",
    "# GLOBAL DICTIONARIES\n",
    "\n",
    "ligi = {\n",
    "    'ekstraklasa':'https://ligafanow.pl/rozgrywki/tabela/30/235',\n",
    "    '1liga':'https://ligafanow.pl/rozgrywki/tabela/30/234',\n",
    "    '2liga':'https://ligafanow.pl/rozgrywki/tabela/30/231',\n",
    "    '3liga':'https://ligafanow.pl/rozgrywki/tabela/30/232',\n",
    "    '4liga':'https://ligafanow.pl/rozgrywki/tabela/30/233',\n",
    "    '5liga':'https://ligafanow.pl/rozgrywki/tabela/30/230',\n",
    "    '6liga':'https://ligafanow.pl/rozgrywki/tabela/30/229',\n",
    "    '7liga':'https://ligafanow.pl/rozgrywki/tabela/30/228',\n",
    "    '8liga':'https://ligafanow.pl/rozgrywki/tabela/30/227',\n",
    "    '9liga':'https://ligafanow.pl/rozgrywki/tabela/30/226',\n",
    "    '10liga':'https://ligafanow.pl/rozgrywki/tabela/30/225',\n",
    "    '11liga':'https://ligafanow.pl/rozgrywki/tabela/30/224',\n",
    "    '12liga':'https://ligafanow.pl/rozgrywki/tabela/30/223',\n",
    "    '13liga':'https://ligafanow.pl/rozgrywki/tabela/30/236'\n",
    "}\n",
    "ligi_strzelcy = {key: value.replace('tabela', 'strzelcy') + '?loadpl=all' for key, value in ligi.items()}\n",
    "\n",
    "# MAIN FUNCTIONS\n",
    "\n",
    "def get_table(league, ligi = ligi):\n",
    "    \"\"\"\n",
    "    Retrieves and parses the league table data from the provided league name using the corresponding URL.\n",
    "    \n",
    "    Parameters:\n",
    "    - league (str): The name of the league for which the table data will be fetched.\n",
    "    - ligi (dict): A dictionary containing league names as keys and their corresponding URLs as values.\n",
    "    \n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame representing the league table, with headers and data extracted from the HTML\n",
    "      content of the league's URL. The DataFrame is cleaned and formatted for further analysis.\n",
    "    \"\"\"\n",
    "    # Get response from url\n",
    "    response = requests.get(ligi[league])\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find table in url\n",
    "    table = soup.find('table')\n",
    "    \n",
    "    # Empty lists for data\n",
    "    headers = []\n",
    "    rows = []\n",
    "    \n",
    "    # Get headers\n",
    "    for th in table.findAll('th'):\n",
    "        headers.append(th.text.strip())\n",
    "        \n",
    "    # Get data\n",
    "    for tr in table.findAll('tr'):\n",
    "        row = []\n",
    "        for td in tr.findAll(['td', 'th']):\n",
    "            row.append(td.text.strip())\n",
    "        rows.append(row)\n",
    "        \n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:]\n",
    "    df = convert_to_int(df)\n",
    "    df = adjust_dataframe(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_matches(league, round_=None, team=None, ligi=ligi):\n",
    "    \"\"\"\n",
    "    Retrieves and compiles the match data for a given league, with optional filters for specific rounds or teams.\n",
    "\n",
    "    Parameters:\n",
    "    - league (str): The name of the league for which match data will be fetched.\n",
    "    - round_ (int or None): If provided, filters the match data to include only matches from the specified round.\n",
    "    - team (str or None): If provided, filters the match data to include only matches involving the specified team.\n",
    "    - ligi (dict): A dictionary containing league names as keys and their corresponding URLs as values.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame representing the match data for the specified league. The DataFrame is cleaned\n",
    "      and formatted for further analysis. Optional filters based on round or team are applied if provided.\n",
    "    \"\"\"\n",
    "    links = table_of_links(league)\n",
    "    wyniki = []\n",
    "    for zespol in links['Zespół']:\n",
    "        wynik = take_table_results(extract_mecze_links(zespol))\n",
    "        # Przykład użycia\n",
    "        wynik['Kol.'] = wynik['Kol.'].apply(extract_round)   \n",
    "        wyniki.append(wynik)\n",
    "    df = pd.concat(wyniki)\n",
    "    df = df.drop_duplicates()\n",
    "    df = convert_to_int(df)\n",
    "    df.sort_values(['Kol.', 'Godz.'], ascending=[False, True], inplace=True)  # Dodano inplace=True\n",
    "    \n",
    "    # When round added\n",
    "    if round_ is not None:\n",
    "        df = df[df['Kol.'] == round_]\n",
    "    \n",
    "    # When team added\n",
    "    if team is not None:\n",
    "        df = df[(df['Gospodarz'] == team) | (df['Gość'] == team)]      \n",
    "    \n",
    "    return df\n",
    "\n",
    "# ASIDE FUNCTIONS\n",
    "\n",
    "def table_of_links(league, ligi = ligi):\n",
    "    \"\"\"\n",
    "    Function that extracts a table with columns containing links from the given URL\n",
    "    \"\"\"\n",
    "    # Get response from URL\n",
    "    response = requests.get(ligi[league])\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table in the URL\n",
    "    table = soup.find('table')\n",
    "\n",
    "    # Empty lists for data\n",
    "    headers = []\n",
    "    link_columns = []\n",
    "\n",
    "    # Get headers\n",
    "    for th in table.findAll('th'):\n",
    "        headers.append(th.text.strip())\n",
    "\n",
    "    # Get data\n",
    "    for tr in table.findAll('tr'):\n",
    "        row = []\n",
    "        for td in tr.findAll(['td', 'th']):\n",
    "            # Check if the td tag contains an 'a' tag (link)\n",
    "            link = td.find('a')\n",
    "            if link:\n",
    "                # If there is a link, append the link's href attribute\n",
    "                row.append(link.get('href'))\n",
    "            else:\n",
    "                # If there is no link, append an empty string\n",
    "                row.append('')\n",
    "        link_columns.append(row)\n",
    "\n",
    "    # Create a DataFrame with link columns\n",
    "    df = pd.DataFrame(link_columns, columns=headers)\n",
    "\n",
    "    # Remove rows with all empty values\n",
    "    df = df.replace('', pd.NA).dropna(how='all')\n",
    "\n",
    "    # Remove columns with all empty values\n",
    "    df = df.replace('', pd.NA).dropna(axis=1, how='all')\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def triple_strings(list_):\n",
    "    \"\"\"\n",
    "    Formats data from league tables by grouping every three consecutive elements with varying space separations into\n",
    "    clear rows. This function is designed to enhance the readability of league table data.\n",
    "\n",
    "    Parameters:\n",
    "    - list_ (list): A list containing strings, typically extracted from league tables, where data elements are\n",
    "      separated by different numbers of spaces.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: A list of formatted strings, with every three consecutive elements joined into clear rows for\n",
    "      improved presentation of league table data.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(0, len(list_), 3):\n",
    "        triple = list_[i:i+3]\n",
    "        result.append(' '.join(triple))\n",
    "    return result\n",
    "\n",
    "def string_divide(string, min_space_number=2):\n",
    "    \"\"\"\n",
    "    Splits a given string into substrings based on consecutive spaces, while allowing for a minimum number of spaces\n",
    "    between substrings. The function aims to identify meaningful segments of the input string, considering a specified\n",
    "    minimum space count to distinguish between substrings.\n",
    "\n",
    "    Parameters:\n",
    "    - string (str): The input string to be divided.\n",
    "    - min_space_number (int): The minimum number of consecutive spaces required to separate substrings. Default is 2.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: A list containing the identified substrings from the input string, excluding leading and trailing\n",
    "      whitespaces. Empty substrings are excluded from the result.\n",
    "    \"\"\"\n",
    "    substrings = []\n",
    "    current_podstring = \"\"\n",
    "    space_number = 0\n",
    "\n",
    "    for sign in string:\n",
    "        if sign == ' ':\n",
    "            if space_number >= min_space_number-1:\n",
    "                substrings.append(current_podstring.strip())\n",
    "                current_podstring = \"\"\n",
    "            elif current_podstring.strip():\n",
    "                current_podstring += ' '\n",
    "            space_number += 1\n",
    "        else:\n",
    "            current_podstring += sign\n",
    "            space_number = 0\n",
    "\n",
    "    if current_podstring.strip():\n",
    "        substrings.append(current_podstring.strip())\n",
    "\n",
    "    substrings = [substring for substring in substrings if substring.strip()]\n",
    "\n",
    "    return substrings\n",
    "\n",
    "def convert_to_int(df):\n",
    "    \"\"\"\n",
    "    Converts columns of a DataFrame to the int type where possible.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): DataFrame to be converted.\n",
    "    \n",
    "    Returns:\n",
    "    - pandas.DataFrame: Modified DataFrame.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        try:\n",
    "            df[column] = df[column].astype(int)\n",
    "        except ValueError:\n",
    "            pass  # If conversion is not possible, proceed to the next column\n",
    "    return df\n",
    "\n",
    "def divide_events(string):\n",
    "    pattern = re.compile(r'([PWR]) \\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}')\n",
    "    # Wyszukiwanie pasujących sekwencji w tekście\n",
    "    matches = pattern.findall(text)\n",
    "\n",
    "    # Gromadzenie rodzajów wydarzeń\n",
    "    event_types = [match[0] for match in matches]\n",
    "\n",
    "    # Tworzenie stringa z rodzajami wydarzeń, oddzielonymi przecinkami\n",
    "    result_string = \",\".join(event_types)\n",
    "    \n",
    "    return result_string\n",
    "\n",
    "def adjust_dataframe(df):\n",
    "    df = df.replace('', pd.NA).dropna(axis=1, how='all')\n",
    "    # Linijka do poprawy jeśli jakieś wartości są uzupełnione\n",
    "    df.columns = ['Poz', 'Zespół', 'Mecze_rozegrane', 'Pkt.', 'Pkt.', 'Z', 'R', 'P', 'BZ', 'BS', '+/-','Forma']\n",
    "    df['Forma'] = df['Forma'].apply(lambda x: divide_events(x))\n",
    "    df = df.sort_values('Poz')\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def team_form_df(df, team):\n",
    "    data = df[df['Zespół'] == team]['Forma'][0] \n",
    "    \n",
    "    # Podzielenie danych na wiersze\n",
    "    rows = re.split(r'([PWR])\\s+(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2})', data)[1:]\n",
    "    # Columns\n",
    "    columns = [\"Typ\", \"Data\", \"Czas\", \"Drużyna 1\", \"Wynik\", \"Drużyna 2\", \"Arena\"]\n",
    "    team_df = pd.DataFrame([row.split()[0:3] + [string_divide(row)[1]] + [string_divide(row)[2]]+ [string_divide(row)[3]] + [string_divide(row)[4]] for row in triple_strings(rows)], columns=columns)\n",
    "    return team_df\n",
    "\n",
    "def extract_mecze_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    mecze_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if 'mecze' in link['href']:\n",
    "            mecze_links.append(link['href'])\n",
    "\n",
    "    return mecze_links[0]\n",
    "\n",
    "def extract_mecze_details_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    mecze_details_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if 'raport' in link['href'] and 'veo' not in link['href']:\n",
    "            mecze_details_links.append(link['href'])\n",
    "\n",
    "    return list(set(mecze_details_links))\n",
    "\n",
    "def take_table_results(url):\n",
    "    \"\"\"\n",
    "    Function that takes a table of league from the given url\n",
    "    \"\"\"\n",
    "    # Get response from url\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find table in url\n",
    "    table = soup.find('table')\n",
    "\n",
    "    # Empty lists for data\n",
    "    headers = []\n",
    "    rows = []\n",
    "\n",
    "    # Get headers\n",
    "    for th in table.findAll('th'):\n",
    "        headers.append(th.text.strip())\n",
    "\n",
    "    # Get data\n",
    "    for tr in table.findAll('tr')[1:]:  # Skip the first row as it contains headers\n",
    "        row = []\n",
    "\n",
    "        # Check if the row contains data or nested structure\n",
    "        if tr.find('td', class_='hideonmobie'):\n",
    "            # Extract data from nested structure\n",
    "            mobile_data = tr.find('span', class_='d-block d-sm-none')\n",
    "            row.append(mobile_data.find('div', class_='text-uppercase').text.strip())  # Extract mobile data\n",
    "            row.extend([td.text.strip() for td in mobile_data.find_all('div', class_='text-center')])\n",
    "\n",
    "        else:\n",
    "            # Extract data from regular structure\n",
    "            row.extend([td.text.strip() for td in tr.find_all(['td', 'th'])])\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Customowa funkcja do wyciągania wartości\n",
    "def extract_round(row):\n",
    "    match = re.search(r'kolejka (\\d+)', row)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def match_details(path):\n",
    "    response = requests.get(path)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Znajdź wszystkie tabelki na stronie\n",
    "    tables = soup.find_all('table')\n",
    "\n",
    "    # Przechowaj ramki danych w liście\n",
    "    dataframes = []\n",
    "\n",
    "    # Sprawdź, czy znaleziono jakiekolwiek tabele\n",
    "    if tables:\n",
    "        # Iteruj przez wszystkie znalezione tabele\n",
    "        for table in tables:\n",
    "            # Wydziel dane z tabeli do listy słowników\n",
    "            table_data = []\n",
    "            rows = table.find_all('tr')\n",
    "\n",
    "            # Sprawdź, czy istnieje przynajmniej jeden wiersz\n",
    "            if rows:\n",
    "                header = [header.text.strip() for header in rows[0].find_all(['th', 'td'])]\n",
    "\n",
    "                # Iteruj przez pozostałe wiersze\n",
    "                for row in rows[1:]:\n",
    "                    row_data = [cell.text.strip() for cell in row.find_all(['th', 'td'])]\n",
    "                    table_data.append(dict(zip(header, row_data)))\n",
    "\n",
    "                # Przekształć dane do DataFrame\n",
    "                df = pd.DataFrame(table_data)\n",
    "                dataframes.append(df)\n",
    "\n",
    "    # Wydrukuj ramki danych\n",
    "    df = pd.concat([dataframes[1],dataframes[2]]).dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "    def extract_table_data(table):\n",
    "    table_data = []\n",
    "    \n",
    "    # Znajdź nagłówki z pierwszego wiersza thead\n",
    "    header_row = table.select('thead tr')[1]  # Use the second row to get headers with tooltip\n",
    "    headers = []\n",
    "\n",
    "    for header in header_row.find_all(['th', 'td']):\n",
    "        if 'tooltip' in header.attrs:\n",
    "            headers.append(header['tooltip'].strip())\n",
    "        else:\n",
    "            headers.append(header.text.strip())\n",
    "\n",
    "    rows = table.select('tbody tr')\n",
    "    \n",
    "    for row in rows:\n",
    "        player_data = {}\n",
    "        columns = row.find_all(['td', 'th'])\n",
    "        \n",
    "        # Iteruj przez kolumny w danym wierszu\n",
    "        for i, col in enumerate(columns):\n",
    "            player_data[headers[i]] = col.text.strip()\n",
    "        \n",
    "        table_data.append(player_data)\n",
    "\n",
    "    return table_data\n",
    "\n",
    "    def extract_team(url):\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response1.text, 'html.parser')\n",
    "\n",
    "        table1 = soup.find('table', {'id': 'mytxablecc'})\n",
    "        table2 = soup.find('table', {'id': 'mytxablec'})\n",
    "\n",
    "        # Sprawdź, czy obie tabele zostały znalezione\n",
    "        if table1 and table2:\n",
    "            # Ekstrahuj dane z obu tabel\n",
    "            data1 = extract_table_data(table1)\n",
    "            data2 = extract_table_data(table2)\n",
    "\n",
    "        # Połącz dane z obu tabel w jedną listę\n",
    "        combined_data = data1 + data2\n",
    "\n",
    "        # Przekształć dane do DataFrame\n",
    "        df = pd.DataFrame(combined_data)\n",
    "        df.columns = ['Imie i nazwisko', 'Numer', 'Liczba wystepów', 'Liczba bramek',\n",
    "           'Asysty', 'Kanadyjcztk', 'Superstar', 'Top6', 'MVP', 'Czerwone kartki',\n",
    "           'Zółte kartki', 'Stracone bramki', 'Samobój', 'Obronione karne',\n",
    "           'Czyste konto', 'Gold Team','ID']\n",
    "        return df\n",
    "    \n",
    "    def reports_links(url):\n",
    "    list_ = extract_mecze_details_links(extract_mecze_links(url)) #dff.Zespół[0])\n",
    "    list_ = ['https://ligafanow.pl/'+ elem for elem in list_]\n",
    "    return list_"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
